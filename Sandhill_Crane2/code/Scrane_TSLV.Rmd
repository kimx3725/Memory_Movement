---
title: "Scrane_TSLV"
author: "Dennis Kim"
date: "2023-02-01"
output: html_document
---

# Load packages 
```{r packages, message =FALSE, warning = FALSE}
# data calling & wrangling 
library(here)
library(knitr)
library(readr)
library(tidyr)
library(purrr)
library(dplyr)
library(forcats)
library(tibble)
library(stringr)
library(broom)
library(lubridate)

# spatial analysis 
library(sf)
library(raster)
library(rgdal)
library(spatialEco)

# movement analysis 
library(amt)

# visualization
library(ggplot2)
library(viridis)

knitr::opts_chunk$set(echo = TRUE)
```

# Load Data
```{r data call}
# read the raw data of the crane 
crane <- read_rds(here("data", "A5_raw.Rdata"))

# change the column names
colnames(crane) <- c("t_", "x_", "y_", "id", "sex", "age", "year", "age_yr")

# select only coordinates and timestamp of the crane 
crane <- crane %>% dplyr::select(c(t_, x_, y_))

# summary of the data
summary(crane)
```

import raster layers 
```{r raster layers}
# NLCD2016 Landcover layers (30m resolution)
NLCD2016 <- raster(here("data/NLCD_2016_Land_Cover", "NLCD_2016_Land_Cover_L48_20190424.img"))

# crop the NLCD raster with the boundary of our interest 
# call the extent 
myExtent <- rgdal::readOGR(here("data/shapefile", "SummerCrane.shp"))

# get the same crs as the NLCD 2016 layer 
myExtent <- spTransform(myExtent, crs(proj4string(NLCD2016)))

# crop the NLCD 2016 layer based on the extent 
new_NLCD16 <- crop(x = NLCD2016, y = myExtent)

# remove the data that are no longer needed for further analysis 
rm(NLCD2016, myExtent)
```

# AMT conversion with habitat predictor 

## amt conversion 

Add a track class to the data and summarize the data 
```{r amt conversion}
# make tracks 
trk.crane <- amt::make_track(crane, .x=x_, .y=y_, .t=t_, crs= CRS("+init=epsg:4326"))

# convert the crs of the trak object as same as the raster layer (use NAD83)
trk.crane <- amt::transform_coords(trk.crane, st_crs(5070))

summary(trk.crane)
```

Summarize the sampling rates of the crane
```{r sampling rates}
# 5 mins sampling seems reasonable 
summarize_sampling_rate(trk.crane) 
```

change the track point to step 
1. Resample track and filter bursts 
2. Convert track to steps 
3. Create random steps (5 random steps)
4. Extract covariate values 
```{r retrk crane}
set.seed(1)

# follow the above approach 
# the filtering needs to come before the simulating random points otherwise there will be random points that do not correspond to a use point in the data
ssfdat.crane <- 
  trk.crane %>% track_resample(rate = minutes(30), tolerance = minutes(5)) %>% 
  steps_by_burst() %>% 
  filter(!is.na(ta_)) %>%
  random_steps() %>% 
  extract_covariates(new_NLCD16)

# summary of the ssf data
summary(ssfdat.crane)
```

Check the reasonable buffer distance for each step length - you can adjust this based on the summary of your step lengths 
```{r sl distribution}
ssfdat.crane %>% filter(case_ == "TRUE") %>% dplyr::select(sl_) %>% summary()
```

# Spatial temporal cognitive map

select randomized and used locations of the tracks from the crane for making a map
```{r st points}
# select random locations that matched with observed step lengths 
obs <- ssfdat.crane %>% filter(case_ == TRUE) # filter the used locations only 
ran <- ssfdat.crane %>% filter(case_ == FALSE) # filter the random locations only 

# observed locations
obs.loc <- obs %>% dplyr::select(x1_, y1_) # select the coordinates 
colnames(obs.loc) <- c("x", "y") # change the coordinates names 
obs.loc.sf <- st_as_sf(obs.loc, coords = c("x", "y"), crs = "+init=epsg:5070") # convert the format to sf object 
obs.loc %>% head() # check the head of the converted sf data 

# random locations (apply the same approaches as above)
ran.loc <- ran %>% dplyr::select(x2_, y2_)
colnames(ran.loc) <- c("x", "y")
ran.loc.sf <- st_as_sf(ran.loc, coords = c("x", "y"), crs = "+init=epsg:5070")
ran.loc %>% head()

# Even if points are not included in the step selection analysis (e.g., because there is a previous data point missing) we must include them in our grid and our subsequent calculation of TSLV, so I have added this extra object that is incorporated into the grid and subsequent steps of the analysis.
all.obs.loc <- trk.crane %>% dplyr::select(x_, y_)
colnames(all.obs.loc) <- c("x", "y")

# make a spatial points object for the SSF data (use and random points combined). This is important for getting the grid_id for every point from the memory.map.sf grid
ssfdat.crane.sf.endpoints <- st_as_sf(ssfdat.crane, coords = c("x2_", "y2_"), crs = "+init=epsg:5070")
```

we will first create a grid which the extent equals to the bounding box of the selected points
```{r memory map}
# create an entire locations as sf including random and observed 
all.loc <- rbind(obs.loc, ran.loc, all.obs.loc) # rbind the observed and random amt locations and raw observed locations together 
all.loc.sf <- st_as_sf(all.loc, coords = c("x", "y"), crs = "+init=epsg:5070") # convert to the sf object 

# create 3000m x 300m grid cell in the map 
memory.map = st_make_grid(all.loc.sf, c(300, 300), what = "polygons", square = TRUE)
memory.map # check the grid map 

# convert the map to sf object
memory.map.sf = st_sf(memory.map)

# plot the map
memory.map.sf %>% plot()
memory.map.sf

# : adding grid_id to memory.map.sf so we can get it for all values at the same grid
memory.map.sf$grid_id = 1:nrow(memory.map.sf)
```

Get grid_id values for the SSF data as well as the original, use-only crane data (which includes some points not included in the step selection analysis but still necessary for accurate TSLV calculation)
```{r grid id values}
# get the grid id for ssf data
ssfdat.grid.id <-  sf::st_intersection(ssfdat.crane.sf.endpoints, memory.map.sf) %>% as.data.frame
# create the same grid id columns on the ssf data
ssfdat.crane$grid_id <- ssfdat.grid.id$grid_id
ssfdat.crane

# get the grid id for the original, use-only crane data
all.obs.loc.sf = st_as_sf(trk.crane, coords = c("x_", "y_"), crs = "+init=epsg:5070")
all.grid.id =  sf::st_intersection(all.obs.loc.sf, memory.map.sf) %>% as.data.frame
trk.crane$grid_id = all.grid.id$grid_id
trk.crane
```

## Time Since Last Vitist (TSLV)

Follow Uli's approach: Their definition of TSLV is short and sweet (see equation 4 in their paper) - basically, it's 0 if the point in question is within some distance Î´ (this value could be similar to the value you used for your buffer) of the previous point, and otherwise it's (previous TSLV + 1). So they define it iteratively, starting at the first point and iteratively updating TSLV with each time step.

General approaches: 
1. create a spatial temporal map with the number of cells (for our cases, we use the 2000m x 2000m buffer).
2. set the burn-in period (365 days - a year) - t index would correlate with the number of observation id 
3. calculate the tslv based on the year observations - at the end of the calculation, all the NAs would replace by 354 
4. continue the same approaches 
5. merge the tslv values to the observed locations 

create a TSLV function
```{r tslv new}
tslv.PRT = function(grid_id, time, prev_data) {
  # grid_id: an integer representing a grid cell for which TSLV is to be calculated
  # time: a point in time for which TSLV is to be calculated
  # prev_data: a data.frame, with columns grid_id and t_. All values of t_ need not be before "time" as this will be filtered inside the function here. This data.frame should only contain points the animal actually visited, even if grid_id represents a random point.
  
  # Get all previous points that are recorded at grid_id
  prev_visits = prev_data[prev_data$grid_id == grid_id & prev_data$t_ < time, ]
  if (nrow(prev_visits) == 0) return(NA) # for now, keep this at NA. We will fix for the burnin later.
  
  last_visit = prev_visits[nrow(prev_visits), ] # get the final row of the data.frame
  
  # Returns a numeric value representing the time difference between last_visit and time
  return(as.numeric(difftime(time, last_visit$t_, units = "hours")))
}
```

calculate TSLV for all values in the SSF data frame
```{r ssf tslv}
# apply the TSLV function
ssfdat.crane$tslv = sapply(X = 1:nrow(ssfdat.crane), FUN = function(row) {
  tslv.PRT(ssfdat.crane$grid_id[row], ssfdat.crane$t2_[row], trk.crane)
})

# create the final ssfdata 
ssfdat.crane.final = ssfdat.crane
ssfdat.crane.final

#save the data for checking, can comment these out and it won't affect results
#write_csv(ssfdat.crane, path = here("data", "ssfdat_crane.csv"))
#write_csv(trk.crane, path = here("data", "trk_crane.csv"))
```

filtering the burn-in phases and replace NA random points TSLV to current time for the random points subtract the burn-in value: 2015-07-15 05:07:41
```{r replace the burn-in values}
# include the column information year for further filtering 
ssfdat.crane.final <- ssfdat.crane.final %>% mutate(year = year(t1_)) 

# replace NA random points TSLV to current time for the random points subtract the burn-in value: 2015-07-15 05:07:41
ssfdat.crane.final1 <- ssfdat.crane.final  %>% dplyr::mutate(tslv = ifelse(is.na(tslv), difftime(t2_,"2015-07-15 05:07:41", units = "mins"), tslv))

# change the NDVI column name 
colnames(ssfdat.crane.final1)[13] <- "Land_Cover"

ssfdat.crane.final1
```

Now, we've got values but more categories for both NLCD landcover layers and CropScape layers. David made a table in excel of all the nlcd values and then categories that I grouped them in. I have made a table in excel of all the crop values and categories. 
```{r land cover legend}
# now add in category names and descriptions 
nlcd <- read_csv(here("data/NLCD_2016_Land_Cover/nlcd_legend.csv"))

# rename 2nd column to 'nlcd' to make join simpler 
colnames(nlcd)[2] <- 'nlcd'
colnames(ssfdat.crane.final1)[13] <- 'nlcd'
ssfdat.crane.final1

# join the actual nlcd habitat name and also a consolidate category from 20 > 8 levels 
ssfdat.crane.final2 <- left_join(ssfdat.crane.final1, nlcd[,1:3])

#now there is a column called 'nlcd' with the original nlcd value.
##column 'name': a word description of the original nlcd value 
##column 'category': a binned version 

# rename the row name "planted_cultivated" to "cultivated"
ssfdat.crane.final2$category[ssfdat.crane.final2$category == "planted_cultivated"] <- "cultivated"

# rename the column names of category and name to nlcd specified
colnames(ssfdat.crane.final2)[21] <- 'nlcd_category'

colnames(ssfdat.crane.final2)[22] <- 'nlcd_name'

# relevel the reference base category 
ssfdat.crane.final2$category <- fct_relevel(ssfdat.crane.final2$category, "developed")
```


## TSLV Visualization
TSLV visualization
[note] the reasonalbe TSLV map should be... 
* high values: occur in the grid cells that are less visited 
* low values: occur in the grid cells that are frequently visited
```{r TSLV visualization}
# overall plot - there is a few steps that has really long TSLV values than the others 
ssfdat.crane.final2

# visualize the tslv 
tslv.visualization <-  ggplot()+
  geom_sf(data = memory.map.sf, fill = "white")+
  #geom_path(data = ssfdat.crane.final2, aes(x=x1_, y=y1_, col = tslv))+
  geom_point(data = ssfdat.crane.final2 %>% filter(case_ == TRUE), aes(x=x1_, y=y1_, col = tslv), size =1.5)+
  scale_color_viridis(option = "viridis")+
  theme_bw()

tslv.visualization

# save the image 
#ggsave("tslv.plot.png", tslv.visualization, width = 12, height = 7, dpi = 700, bg = NA)
```

## FitSSF

```{r fit ssf}
# only environmental predictor 
ssfdat.crane.final2 %>% filter(t1_ > "2016-07-15 05:07:41") %>% amt::fit_issf(case_ ~ nlcd+ log(sl_)+ cos(ta_)+ strata(step_id_)) %>% summary()

# only tslv predictor 
ssfdat.crane.final2 %>% filter(t1_ > "2016-07-15 05:07:41") %>% amt::fit_issf(case_ ~ nlcd+ tslv+ category:tslv+ log(sl_)+ cos(ta_)+ strata(step_id_)) %>% summary()
```
save the filtered data
```{r save the filtered data}
#write_csv(ssfdat.crane.final2, path = here("data", "ssfdat.crane.final.tslv.csv"))
```
## Model fit visualization

store coefficents and confidence intervals (CIs) in the restructured data for the visualization. 
```{r visualization}
# name the model fit to the df 
tslv.model <- ssfdat.crane.final2 %>% filter(t1_ > "2016-07-15 05:07:41") %>% amt::fit_issf(case_ ~ nlcd+ tslv+ category:tslv+ log(sl_)+ cos(ta_)+ strata(step_id_))
tslv.model

# get the coefficient values
coef.data <- tslv.model$model$coefficients %>% tidy() %>% 'colnames<-'(c("beta", "coefficient"))
coef.data

# get the confidence intervals 
ci.data <- tslv.model$model %>% confint() %>% data.frame() %>% 'colnames<-'(c("CI2.5", "CI97.5")) %>% mutate(beta = coef.data$beta)
ci.data

model.fit.data <- left_join(coef.data, ci.data)
model.fit.data
```

Create a coefficient plot 
```{r coef plot}
model.fit.data1 <- model.fit.data %>% filter(., grepl("category", beta))
model.fit.data1$beta <- c("cultivated", "forest", "herbaceous", "shrubland", "water", "wetlands")
model.fit.data1

landcover_tslv <- ggplot(model.fit.data1, aes(coefficient, beta, color = factor(beta)))+
  geom_vline(xintercept = 0, lty=2, lwd=1, colour="grey50")+
  geom_point(size=3, pch=20, position=position_dodge(width=0.5))+
  geom_linerange(aes(xmax = CI2.5, xmin = CI97.5))+
  theme_bw()+
  xlab("Coefficient Effect size")+
  theme(legend.position = "none", axis.title.y = element_blank())+
  ggtitle("Land Cover Class x TSLV")

landcover_tslv

# save the image 
ggsave("lc_tslv.plot.png", landcover_tslv, width = 5, height = 3, dpi = 700, bg = NA)
```


# Document Footer 
```{r session info}
sessionInfo()
```